{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebooks illustrates a pipeline for extracting overrepresented k-mers in set of sequences also called peaks.<br/>\n",
    "Peaks are supposed to be produced by ChIP-seq experiments and they are examined in order to extract transcription factor binding sites (TFBS).<br/>\n",
    "In the current form, the pipeline contains an initial procedure to extract variable-length words that are present in at least a given percentage of peaks. <br/>\n",
    "The extracted words are then scored by means of different scoring procedures that are also compared each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General notation\n",
    "\n",
    "We denote with $S$ the input set of **peaks**, that are strings in the genomic **alphabet** $\\Gamma = \\{A,C,G,T\\}$ .<br>\n",
    "$S_i$ is th $i$-th peak, and they are ordered by their position in the reference genome.\n",
    "\n",
    "We denote with $D(S)$ the complete set of substrings in $S$, of any length.<br>\n",
    "Moreover, we denote with $D_K(S)$ the complete set of $k$-mers (words of a specific length $k$) that appeat at least once in $S$.\n",
    "\n",
    "Given a word $w$ and a set of strings $S$, the **document listing problem** consists in enumerating the strings were $w$ occurrs at least once.<br>\n",
    "We define $dl(w,S)$ the document listing of $w$ in $S$, also called the **support** of $w$ in $W$, namely the list of strings of $S$ where $w$ occurrs.<br>\n",
    "\n",
    "We denote with $ml(w,S)$ the total number of occurrences of $w$ in S, and with $ml(w,S_i)$ the number of occurrences of $w$ within the specific string $S_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# A data structure to bring them all\n",
    "In what follows, an enhaced suffix array (**ESA**) is used as main data structure oin top of which algirhtms are built.<br>\n",
    "Wew remind that the ESA data structure is composed of the suffix arry $SA$ and the longest common prefix array $LCP$.<br>\n",
    "Teh ESA structure is built for the set of sequences $S$, thus it represents all the suffixes of the strings in $S$.<br>\n",
    "Teh structure is built on the concatenation of the strings in $S$ that is in the form $\\mathbb{S} = S_1\\$S_2\\$\\dots S_i\\$S_{i+1}\\$\\dots \\$S_n\\$$.<br>\n",
    "\n",
    "The followiing source code describes as a set of input sequences in FASTA format are read and concatenated into a global string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_global_sequence(ifile, first_n=None):\n",
    "    \"\"\"\n",
    "    Read a set of sequences in FASTA format and concatenat them into a global squences.\n",
    "    Returns:\n",
    "        - the original list of strings\n",
    "        - a dict which maps sequence names (defined by the comment rows in the FASTA file) to their ID (defined by the order in which they are readed, starting from 0)\n",
    "        - the global sequence ibtained by concatenation\n",
    "    \"\"\"\n",
    "    gs = ''\n",
    "    iline = 0\n",
    "    seqname = ''\n",
    "    seqname2id = dict()\n",
    "    ipeaks = list()\n",
    "    for line in open(ifile, 'r'):\n",
    "        if iline%2 == 0:\n",
    "            seqname = line.strip()\n",
    "            seqname2id[seqname] = iline/2\n",
    "        else:\n",
    "            s = line.strip().upper().replace('N','$') + '$'\n",
    "            ipeaks.append( (s, seqname) )\n",
    "            gs += s\n",
    "        iline += 1\n",
    "        if first_n!=None and iline/2==first_n:\n",
    "            break\n",
    "    return ipeaks, seqname2id, gs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ESA structure is enriched with two additional arrays:\n",
    "<ul>\n",
    "    <li>the <b>DL array</b>  which reports for each suffix the string in $S$ to which it belongs to</li>\n",
    "    <li>the <b>NN array</b> that is used in case the input string contains ambiguous characters $N$. This array is used in order to discard k-mer which contains $N$ characters in an efficient way</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following source code defined the given data structure, also called **DNESA**, and an iterator that can be used for enumerating all the $k$-mers contained in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNESA:\n",
    "    def __init__(self, _sa, _lcp, _nn, _dl, _gs):\n",
    "        self.sa = _sa\n",
    "        self.lcp = _lcp\n",
    "        self.nn = _nn\n",
    "        self.dl = _dl\n",
    "        self.gs  = _gs\n",
    "        \n",
    "    def search(self, w):\n",
    "        l = 0\n",
    "        r = len(self.gs)-1\n",
    "        #ll,lr,rl,rr,m,cq\n",
    "        for i in range(len(w)):\n",
    "            #print('i',i)\n",
    "            \n",
    "            \"\"\"\n",
    "            while (l<len(self.nn)) and (self.nn[l] < len(w)-i):\n",
    "                l += 1\n",
    "            if l>r or (self.gs[self.sa[l]+i] > w[i]):\n",
    "                return None,None\n",
    "            \n",
    "            while (r>l) and ( self.nn[r]<len(w)-i ):\n",
    "                r -= 1\n",
    "            if r<l or (self.gs[ self.sa[r]+i]<w[i] ):\n",
    "                return None,None\n",
    "            \n",
    "            #print('new lr',l,r)\n",
    "            \"\"\"\n",
    "            \n",
    "            ll = l\n",
    "            lr = r\n",
    "            while ll!=lr:\n",
    "                m = ll + int( (lr-ll)/2 )\n",
    "                if w[i] <= self.gs[self.sa[m]+i]:\n",
    "                    lr = m\n",
    "                else:\n",
    "                    ll = m+1\n",
    "            l = ll\n",
    "            #print('l',l)\n",
    "            if l>r or w[i]!=self.gs[self.sa[l]+i]:\n",
    "                return None,None\n",
    "            \n",
    "            #print('new l',l)\n",
    "            \n",
    "            rl = l\n",
    "            rr = r\n",
    "            #print(rl,rr)\n",
    "            while rl!=rr:\n",
    "                m = rl + int( (rr-rl)/2 )\n",
    "                #print(rl,rr,m)\n",
    "                if (rr-rl)%2 != 0:\n",
    "                    m += 1\n",
    "                if w[i] < self.gs[self.sa[m]+i ]:\n",
    "                    rr = m-1\n",
    "                else:\n",
    "                    rl = m\n",
    "            r = rr\n",
    "            if r>len(self.sa) or self.gs[self.sa[r]+i] !=w[i]:\n",
    "                return None,None\n",
    "        \n",
    "        return l,r+1\n",
    "\n",
    "\n",
    "\n",
    "class NESAIterator:\n",
    "    __s = None\n",
    "    __k = 0\n",
    "    __sa = None\n",
    "    __lcp = None\n",
    "    __ns = None\n",
    "    __i = 0\n",
    "    __j = 0\n",
    "    \n",
    "    def __init__(self, s, k, sa , lcp, ns):\n",
    "        self.__s = s\n",
    "        self.__k = k\n",
    "        self.__sa = sa\n",
    "        self.__lcp = lcp\n",
    "        self.__ns = ns\n",
    "\n",
    "    def get_k(self):\n",
    "        return self.__k\n",
    "    \n",
    "    def reset(self):\n",
    "        self.__i = 0\n",
    "        self.__j = 0\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self.__i < len(self.__s):\n",
    "            self.__i = self.__j\n",
    "            \n",
    "            while (self.__i < len(self.__s)) and  ( (self.__sa[self.__i] > len(self.__s) - self.__k - 1) or (self.__ns[self.__i] < self.__k) ):\n",
    "                self.__i += 1\n",
    "            if self.__i == len(self.__s):\n",
    "                raise StopIteration\n",
    "            self.__j = self.__i+1\n",
    "            while ( self.__j < len(self.__s) ) and (self.__lcp[self.__j] >= self.__k) and (self.__ns[self.__i] >= self.__k) :\n",
    "                self.__j += 1\n",
    "            ret = self.__s[ self.__sa[self.__i] : self.__sa[self.__i] + self.__k ]\n",
    "            #self.__i = self.__j #!!!!!!\n",
    "            return ret\n",
    "        else:\n",
    "            raise StopIteration\n",
    "            \n",
    "    def get_start(self):\n",
    "        return self.__i\n",
    "    \n",
    "    def get_end(self):\n",
    "        return self.__j\n",
    "    \n",
    "    def multiplicity(self):\n",
    "        return self.__j - self.__i\n",
    "    \n",
    "    def positions(self):\n",
    "        return self.__sa[self.__i : self.__j]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following source code provides a procedure to build the DNESA structure starting from an input set of sequences in FASTA format.<br>\n",
    "The structure is write into a CSV file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "def build_DNESA(i_sequences_file, o_dataframe_file, first_n=None):\n",
    "    ifile = i_sequences_file\n",
    "    print('reading sequences')\n",
    "    iline = 0\n",
    "    seqname = ''\n",
    "    seqname2id = dict()\n",
    "    ipeaks = list()\n",
    "    for line in open(ifile, 'r'):\n",
    "        if iline%2 == 0:\n",
    "            seqname = line.strip()\n",
    "            seqname2id[seqname] = iline/2\n",
    "        else:\n",
    "            ipeaks.append( (line.strip().upper().replace('N','$') + '$', seqname) )\n",
    "        iline += 1\n",
    "        if first_n!=None and iline/2==first_n:\n",
    "            break\n",
    "            \n",
    "    print('making global sequence')\n",
    "    global_sequence = ''\n",
    "    suffixes = list()\n",
    "    for i in range(len(ipeaks)):\n",
    "        peak = ipeaks[i][0]\n",
    "        seqname = ipeaks[i][1]\n",
    "\n",
    "        for p in range(len(peak)):\n",
    "            suffixes.append( (len(global_sequence) + p, i) )\n",
    "        global_sequence += peak\n",
    "\n",
    "    print('seqname2id', len(seqname2id))\n",
    "    print('global_sequence', len(global_sequence))\n",
    "    print('suffixes', len(suffixes))\n",
    "    \n",
    "    print('ordering suffixes')\n",
    "    def compare_suffixes(t1, t2):\n",
    "        #print('comparing',t1,t2)\n",
    "        i = t1[0]\n",
    "        j = t2[0]\n",
    "        l = len(global_sequence)\n",
    "        k = 0\n",
    "        while (i+k<l and j+k<l and global_sequence[i+k]==global_sequence[j+k]):\n",
    "            k += 1\n",
    "        if i+k==l:\n",
    "            return -1\n",
    "        elif j+k==l:\n",
    "            return 1\n",
    "        else:\n",
    "            #print('@', ord(global_sequence[i+k]) - ord(global_sequence[j+k]))\n",
    "            return ord(global_sequence[i+k]) - ord(global_sequence[j+k])\n",
    "\n",
    "    from functools import cmp_to_key\n",
    "    suffixes = sorted(suffixes, key=cmp_to_key(compare_suffixes) )\n",
    "\n",
    "    sa = [ i[0] for i in suffixes]\n",
    "    dl = [ i[1] for i in suffixes]\n",
    "\n",
    "    for i in range(10):\n",
    "        print(suffixes[i], global_sequence[suffixes[i][0]: suffixes[i][0]+30 ])\n",
    "      \n",
    "    print('creating LCP array')\n",
    "    \n",
    "   \n",
    "    length = len(global_sequence)\n",
    "    rank = [0 for i in range(length)]\n",
    "    for i in range(length):\n",
    "        rank[ sa[i] ] = i\n",
    "    lcp = [0 for i in range(length)]\n",
    "    h = 0\n",
    "    for i in range(length):\n",
    "        k = rank[i]\n",
    "        if k==0:\n",
    "            lcp[k] = -1\n",
    "        else:\n",
    "            j = sa[k-1]\n",
    "            while (i+h<length) and (j+h<length) and (global_sequence[i+h] == global_sequence[j+h] ):\n",
    "                h += 1\n",
    "            lcp[k] = h\n",
    "        if h>0:\n",
    "            h -= 1\n",
    "    \n",
    "    print('creating NN array')\n",
    "    length = len(sa)\n",
    "    nn = [0 for i in range(length)]\n",
    "    pn = length\n",
    "    for i in range(length-1, -1, -1):\n",
    "        if global_sequence[i]=='N' or global_sequence[i]=='$':\n",
    "            nn[i] = 0\n",
    "            pn = i\n",
    "        else:\n",
    "            nn[i] = pn -i\n",
    "    fn = [0 for i in range(length)]\n",
    "    for i in range(length):\n",
    "        fn[i] = nn[sa[i]]\n",
    "    nn = fn\n",
    "    \n",
    "    i = 0\n",
    "    while global_sequence[sa[i]] == '$':\n",
    "        i += 1\n",
    "\n",
    "    #for i in range(i, i+10):\n",
    "    #    print(global_sequence[suffixes[i][0]: suffixes[i][0]+30 ], global_sequence[sa[i]: sa[i]+30 ], sa[i], lcp[i], nn[i], dl[i])\n",
    "        \n",
    "    print('saving  dataframe to csv', o_dataframe_file)\n",
    "    import pandas\n",
    "    df = pandas.DataFrame()\n",
    "    df['sa'] = sa\n",
    "    df['lcp'] = lcp\n",
    "    df['nn'] = nn\n",
    "    df['dl'] = dl\n",
    "    df.to_csv(o_dataframe_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of how to build a DNESA structure, save it and finally reload it into memory.<br>\n",
    "Pandas DataFrame columns are converted into lists in order to increase the efficiency when using them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building and writing\n",
    "build_DNESA('ENCFF682EOV_allseqs.fa', 'ENCFF682EOV_allseqs.nelsa.csv')\n",
    "\n",
    "# reading\n",
    "epeaks, es2n, egs = read_global_sequence('ENCFF682EOV_allseqs.fa')\n",
    "edf = pandas.read_csv('ENCFF682EOV_allseqs.nelsa.csv')\n",
    "print(len(epeaks), len(es2n), len(egs))\n",
    "e_dnesa = DNESA(list(edf.sa), list(edf.lcp), list(edf.nn), list(edf.dl), egs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Plotting procedure\n",
    "This section simply report the plotting procedure used to make scatter plots of out data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_2d_scatter(df, col1, col2, plotsize=(5,5), up_hist_logscale=False):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    left, width = 0.1, 0.65\n",
    "    bottom, height = 0.1, 0.65\n",
    "    spacing = 0.001\n",
    "    rect_scatter = [left, bottom, width, height]\n",
    "    rect_histx = [left, bottom + height + spacing, width, 0.2]\n",
    "    rect_histy = [left + width + spacing, bottom, 0.2, height]\n",
    "\n",
    "    x = df[col1]\n",
    "    y = df[col2]\n",
    "\n",
    "    plt.figure(figsize=plotsize)\n",
    "    ax_scatter = plt.axes(rect_scatter)\n",
    "    ax_scatter.tick_params(direction='in', top=True, right=True)\n",
    "    plt.xlabel(col1)\n",
    "    plt.ylabel(col2)\n",
    "    ax_histx = plt.axes(rect_histx)\n",
    "    ax_histx.tick_params(direction='in', labelbottom=False)\n",
    "    if up_hist_logscale:\n",
    "        ax_histx.set_yscale('log')\n",
    "    ax_histy = plt.axes(rect_histy)\n",
    "    ax_histy.tick_params(direction='in', labelleft=False, labelbottom=False, labeltop=False)\n",
    "\n",
    "    ax_scatter.scatter(x, y, marker='x')\n",
    "    ax_histx.margins(x=0,y=0.1, tight=True)\n",
    "    ax_histx.hist(x, bins=20)\n",
    "    #ax_histx.margins(0)\n",
    "    ax_histx.margins(x=0,y=0.1, tight=True)\n",
    "    ax_histy.hist(y, orientation='horizontal', bins=20)\n",
    "    ax_histy.margins(x=0.1,y=0, tight=True)\n",
    "    #ax_histy.set_xticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# k-mer score over PWM matrices (score and norm_score)\n",
    "\n",
    "This section describes a procedure to calculate a score for a given k-mer over a PWM matrix of length $l$, when $ k \\leq l$.<br>\n",
    "Given an alphabet $\\Sigma = \\{ A,C,G,T \\}$, a Position weight matrix (PWM) $M$ of length $|M|$ is a function $ f : \\Sigma \\times |M| \\mapsto \\mathbb{N} $.<br>\n",
    "Thus, if we think about the PWM as a matrix where rows are indexed by symbols in $\\Sigma$, each column represent a given position of the bindsing site.<br>\n",
    "Moreover, $M$ is built such that the sum over columns are ensured to be equalt. Thus, for every $1 \\leq i \\leq |M|$, $\\sum_{a \\in \\Sigma} M[a][i] = x$, for a given value of $x$.\n",
    "\n",
    "Given a $k$-mer $w$ having the same length of $M$, the score of $w$ is given by $s(w,M) = \\sum_{1 \\leq i \\leq |M|}M[ w[i] ][i]$, being $w[i]$ the $i$-th character of $w$.<br>\n",
    "This score can be normalized by taking into account the maximum possbile score that can be obtained in the PWM.\n",
    "\n",
    "If the length of the k-mer is less than the length of the PWM, then we are interested in finding the position $i$ where a maximum score for the k-mer can be obtained.<br>\n",
    "Thus, we want to find the position $i$ which maximizes the score $s(w,M,i) = \\sum_{i \\leq j \\leq i+|w|}M[ w[j-i] ][i]$, for $1 \\leq i \\leq |M| - |w|$.<br>\n",
    "Therefore, for a k-mer $w$, such that $|w|\\leq|M|$, $s(w,M) = max_{s(w,M,i)}\\{ $ for $ 1 \\leq i \\leq |M| - |w| \\}$.<br>\n",
    "Moreover, this score can be normalized by taking into account the maximum score that can be obtained in the PWM by scanning for a unspecified word of length $|w|$, that is $ max_{s(t,M,i)}\\{$ for $ t \\in \\Sigma^{|w|} $, for $ 1 \\leq i \\leq |M| - |w| \\}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def read_jaspar(ifile):\n",
    "    \"\"\"\n",
    "    read a PWM matrix in jaspar format\n",
    "    \"\"\"\n",
    "    motif = dict()\n",
    "    for line in open(ifile,'r'):\n",
    "        if line[0]!='>':\n",
    "            line = line.strip()\n",
    "            c = line[:line.index('[')].strip()\n",
    "            motif[c] = list()\n",
    "            s = re.split('\\s+', line.strip())\n",
    "            for i in range(2, len(s)-1):\n",
    "                motif[c].append( int(s[i]) )\n",
    "    return motif\n",
    "\n",
    "def get_full_motif_score(pwm, word):\n",
    "    \"\"\"\n",
    "    get the unnormalized score over a PWM matrix M for a word w wuch that |M| = |w|\n",
    "    \"\"\"\n",
    "    score = 0.0\n",
    "    for i in range(len(word)):\n",
    "        score += pwm[word[i]][i]\n",
    "    return score\n",
    "\n",
    "def get_normalized_motif_score(m, w):\n",
    "    \"\"\"\n",
    "    get the uormalized score over a PWM matrix M for a word w wuch that |M| >= |w|\n",
    "    \"\"\"\n",
    "    lm = len(m[next(iter(m.keys()))])\n",
    "    max_score = 0.0\n",
    "    w_score = 0.0\n",
    "    t =  [ max( [ m[k][i] for k in m.keys() ] ) for i in range(lm) ] \n",
    "    for i in range(lm-len(w)+1):\n",
    "        ms = sum( t[i:i+len(w)] )\n",
    "        if ms > max_score:\n",
    "            max_score = ms\n",
    "        ws = 0.0\n",
    "        for j in range(len(w)):\n",
    "            ws += m[w[j]][i+j]\n",
    "        if w_score < ws:\n",
    "            w_score = ws\n",
    "    return w_score/max_score, w_score, max_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some statistics regarding an already known PWM of CTCF\n",
    "\n",
    "In what follows we try to understand how the k-mers generated via an already built PWM for the binding sites of CTCF match in a given set of sequences.<br>\n",
    "In particular, we study the diffusion (suppport) of k-mers within the set of sequences in relation to the score that each k-mer gets in the PWM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "epeaks, es2n, egs = read_global_sequence('ENCFF682EOV_allseqs.fa')\n",
    "edf = pandas.read_csv('ENCFF682EOV_allseqs.nelsa.csv')\n",
    "print(len(epeaks), len(es2n), len(egs))\n",
    "e_dnesa = DNESA(list(edf.sa), list(edf.lcp), list(edf.nn), list(edf.dl), egs)\n",
    "\n",
    "ctcf = read_jaspar('MA0139.1.jaspar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "k = len(ctcf['A'])\n",
    "print('k',k)\n",
    "it = NESAIterator( egs, k, list(edf.sa) , list(edf.lcp), list(edf.nn))\n",
    "dl = list(edf.dl)\n",
    "\n",
    "words = list()\n",
    "scores = list()\n",
    "dls = list()\n",
    "mults = list()\n",
    "\n",
    "for kmer in it:\n",
    "    #print(kmer, it.multiplicity(), it.positions())\n",
    "    #print(it.multiplicity(), it.positions())\n",
    "    #print(kmer, it.multiplicity())\n",
    "    words.append(kmer)\n",
    "    mults.append( it.multiplicity())\n",
    "    scores.append( get_full_motif_score(ctcf, kmer)  )\n",
    "    dls.append( len(  set(  dl[ it.get_start() : it.get_end() ]  ) ) )\n",
    "    \n",
    "epwmstats = pandas.DataFrame()\n",
    "epwmstats['word'] = words\n",
    "epwmstats['score'] = scores\n",
    "epwmstats['dl'] = dls\n",
    "epwmstats['mult'] = mults\n",
    "epwmstats.sort_values('score', ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "display(epwmstats)\n",
    "print('max dl value', max(epwmstats['dl']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epwmstats.to_csv('kmers-stats_ENCFF682EOV_allseqs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "print(statistics.mean(epwmstats[ epwmstats.dl > 1]['dl']))\n",
    "\n",
    "print(epwmstats.groupby('dl').size())\n",
    "\n",
    "print(epwmstats.groupby(['dl']).agg(['mean']))\n",
    "\n",
    "print(max(epwmstats.dl), len(set(edf.dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d_scatter(epwmstats, 'dl', 'score', plotsize=(10,10), up_hist_logscale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "k = 4\n",
    "print('k',k)\n",
    "it = NESAIterator( egs, k, list(edf.sa) , list(edf.lcp), list(edf.nn))\n",
    "dl = list(edf.dl)\n",
    "\n",
    "words = list()\n",
    "scores = list()\n",
    "dls = list()\n",
    "mults = list()\n",
    "\n",
    "for kmer in it:\n",
    "    #print(kmer, it.multiplicity(), it.positions())\n",
    "    #print(it.multiplicity(), it.positions())\n",
    "    #print(kmer, it.multiplicity())\n",
    "    words.append(kmer)\n",
    "    mults.append( it.multiplicity())\n",
    "    scores.append( get_normalized_motif_score(ctcf, kmer)[0]  )\n",
    "    dls.append( len(  set(  dl[ it.get_start() : it.get_end() ]  ) ) )\n",
    "    \n",
    "epwmstats = pandas.DataFrame()\n",
    "epwmstats['word'] = words\n",
    "epwmstats['score'] = scores\n",
    "epwmstats['dl'] = dls\n",
    "epwmstats['mult'] = mults\n",
    "epwmstats.sort_values('score', ascending=False, inplace=True)\n",
    "\n",
    "\n",
    "display(epwmstats)\n",
    "print('max dl value', max(epwmstats['dl']))\n",
    "\n",
    "plot_2d_scatter(epwmstats, 'dl', 'score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Two background models\n",
    "\n",
    "In order to assign a score to a k-mer that has been extracted by a given procedure, we can take into account the divergenge from the real case to a *random* one, also called background model.<br>\n",
    "Here two different background models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nucleotide shuffling procedure\n",
    "The first one is computed by randomly shuffling the nucleotides in each sequence $S_i$, up to a given percentage. This model ensures that the 1-mer content of input sequences are preserved, however it may disrupt the k-mer composition for $k>2$. In fact, it is possible that, if sequences $S_i$ belong to a genome $G$, that this procedure may produce k-mers that do not belong to $G$. On the contrary, a pro of this method is that it try to preserve the context form which the sequences has been extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "iseqs = 'ENCFF682EOV_allseqs.fa'\n",
    "oseqs = 'ENCFF682EOV_allseqs_shuff.fa'\n",
    "\n",
    "shuff_perc = 0.2\n",
    "\n",
    "def shuffle_seq(s):\n",
    "    s = [ c for c in s.upper()]\n",
    "    positions = [ i for i in range(len(s)-1) if s[i]!='N' ]\n",
    "    for n in range( math.ceil(len(positions)*shuff_perc) ):\n",
    "        i = random.randrange(0, len(positions) )\n",
    "        j = random.randrange(0, len(positions) )\n",
    "        t = s[i]\n",
    "        s[i] = s[j]\n",
    "        s[j] = t\n",
    "    return ''.join(s)\n",
    "\n",
    "with open(oseqs,'w') as off:\n",
    "    iline = 0\n",
    "    for line in open(iseqs,'r'):\n",
    "        if iline%2==0:\n",
    "            off.write(line)\n",
    "        else:\n",
    "            s = shuffle_seq(line)\n",
    "            off.write(s)\n",
    "            if iline < 20:\n",
    "                print(line[:100], s[:100])\n",
    "        iline +=1\n",
    "    \n",
    "print(iline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_DNESA('ENCFF682EOV_allseqs_shuff.fa', 'ENCFF682EOV_allseqs_shuff.nelsa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence re-extraction procedure \n",
    "A second procedure randomly extracts sequences from a genome $G$ by preserving the length of the sequences extracted in position $i$. In this way, the new sequences are originated formt he genome $G$, thus it is ensured that every k-mers occuring in them is a *real* k-mer. On the contrary, no restriction is ensured regarding the context from which they are extracted.\n",
    "\n",
    "This procedure is made by using the bed tools software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_DNESA('ENCFF682EOV_allseqs_random.fa', 'ENCFF682EOV_allseqs_random.nelsa.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# A ranking procedure for k-mers based on KL divergence terms (kl_rank and norm_kl_rank)\n",
    "\n",
    "This section shows a methodology to rank k-mers, being all of the same length $k$, by taking into account their single contribution to the calculation of a Kullback-Leibler (KL) divergence form a real case, also called foreground, and a background model $B$.<br>\n",
    "Given a word length $k$, the procedure takes into account the multiplicities of the k-mers occurring at least one in the foreground sequence set $S$.<br>\n",
    "The KL divergence is calculated as $ \\sum_{w \\in D_k(S)}p(w,S) log \\frac{p(w,S)}{p(w,B)} $, where p(w,S) is the frequency of $w$ in $S$, that is computed as $\\frac{ml(w,S)}{ \\sum_i{|S_i|-|w|+1}}$ if sequences in $S$ do not contains $N$ characters.<br>\n",
    "For practical applicatios, it is possible that a k-mer does not occurr in the backgorund set $B$, in this case a very small value is assigned to it and it is ensured that $\\sum_{w \\in D_k(S)}p(w,B) = 1$.<br>\n",
    "Givne a k-mer, its rank is caluclated by ordering the k-mers by their values such that  $kl\\_rank(w,S,B) = p(w,S) log \\frac{p(w,S)}{p(w,B)}$. In this way a k-mer that is over-represented in the foregound w.r.t the background set has a positive contribution, and k-mers that are under-represented have negative contributions.\n",
    "\n",
    "In addition, the $kl\\_rank$ values can be normalized to be values in $[0,1]$ by taking into account the minimum value $kl_min = min_{w \\in D_k(S)} kl\\_rank(w,S,B)$ and the maximum value $kl_max = max_{w \\in D_k(S)} kl\\_rank(w,S,B)$, such that $norm\\_kl\\_rank(w,S,B) = \\frac{kl\\_rank(w,S,B)  - kl\\_min}{kl\\_max - kl\\_min} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_kl_scores(k, egs, edf, rgs, rdf):\n",
    "    f_kmers = dict()\n",
    "\n",
    "    it =  NESAIterator( egs, k, list(edf.sa) , list(edf.lcp), list(edf.nn))\n",
    "    for kmer in it:\n",
    "        f_kmers[kmer] = it.multiplicity()\n",
    "\n",
    "    b_kmers = dict()\n",
    "\n",
    "    it =  NESAIterator( rgs, k, list(rdf.sa) , list(rdf.lcp), list(rdf.nn))\n",
    "    for kmer in it:\n",
    "        b_kmers[kmer] = it.multiplicity()\n",
    "\n",
    "    kmers = f_kmers.keys() | b_kmers.keys()\n",
    "\n",
    "    for kmer in kmers:\n",
    "        f_kmers[kmer] = f_kmers.get(kmer,0)+1\n",
    "        b_kmers[kmer] = b_kmers.get(kmer,0)+1\n",
    "\n",
    "    f_sum = sum(f_kmers.values())\n",
    "    b_sum = sum(b_kmers.values())\n",
    "\n",
    "    scores = dict()\n",
    "    sorted_scores = list()\n",
    "    for kmer in kmers:\n",
    "        f_kmers[kmer] = f_kmers[kmer]/f_sum\n",
    "        b_kmers[kmer] = b_kmers[kmer]/b_sum\n",
    "        scores[kmer] = f_kmers[kmer] * math.log( f_kmers[kmer] / b_kmers[kmer] )\n",
    "        sorted_scores.append((scores[kmer],kmer))\n",
    "    sorted_scores = sorted(sorted_scores, reverse=True)\n",
    "    \n",
    "    min_score = min(scores.values())\n",
    "    max_score = max(scores.values())\n",
    "    \n",
    "    norm_sorted_scores = list()\n",
    "    for s in sorted_scores:\n",
    "        norm_sorted_scores.append(  (  (s[0]-min_score)/(max_score - min_score), s[1] )  )\n",
    "        \n",
    "    return sorted_scores, norm_sorted_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# A DL-aware scoring procedure for k-mers of variable length (norm_kl_score)\n",
    "This section shows a scoring procedure that can be used ot recognize statistically over-represented k-mers in a set of sequence, such that the computation of the score for a specific k-mer does not depend form the computaiton of the score of any other k-mer.<br>\n",
    "In addiction, this procedure enables to take into account the contribution that each sequence in the input set $S$ has to the multiplicity of the k-mer.<br>\n",
    "The only constraint is that $|S_i| = |B_i|$.<br>\n",
    "Given a k-mer $w \\in D_k(G)$, the score of $w$ is calculated by normalizing the divergence $KL(w,S,B) = \\sum_i{ p(w,S_i) log \\frac{p(w,S_i)}{p(w,B_i)} }$ for the maximum value of KL that the distribution $p(w,S_i)$ can reach.<br>\n",
    "In order to estimate the maximum KL value for the distribution $p(w,S_i)$, we takes into account a reordering of the sequences $S_i$, referred to as $\\widehat{S}$, such that $p(w,\\widehat{S_i})\\geq p(w,\\widehat{S_{i+1}})$, and a reodering $\\bar{S}$ such that $p(w,\\bar{S_i})\\leq p(w,\\bar{S_{i+1}})$.<br>\n",
    "It can be shown that the given score is not the maximum that the KL divergence can reach, when $p(w,S_i), p(w,B_i) \\neq 0$, however, we can empirically show that it is a good upperbound to the KL value in practical applications.<br>\n",
    "Finally, the score for a given k-mer $w$ is $norm\\_kl\\_score(w,S,B) = \\frac{KL(w,S,B)}{KL(w,\\widehat{S},\\bar{S})}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_m_distr(dnesa, istart, iend):\n",
    "    distr = dict()\n",
    "    for i in range(istart, iend):\n",
    "        distr[ dnesa.dl[i] ] = distr.get(dnesa.dl[i], 0)+1\n",
    "    return distr\n",
    "\n",
    "def m_dsitr_to_seq_freq(m_distr, seq_lengths, k):\n",
    "    f_distr = dict()\n",
    "    for s,v in m_distr.items():\n",
    "        f_distr[s] = v/( seq_lengths[s]-k+1 )\n",
    "    return f_distr\n",
    "\n",
    "def make_prob_distr(idistr):\n",
    "    odistr = dict()\n",
    "    s = sum(idistr.values())\n",
    "    for k,v in idistr.items():\n",
    "        odistr[k] = v/s\n",
    "    return odistr\n",
    "\n",
    "def add_unzeros(idistr, max_k, miss_value=0.0000001):\n",
    "    odistr = dict()\n",
    "    for i in range(max_k):\n",
    "        odistr[i] = idistr.get(i, miss_value)\n",
    "    return odistr\n",
    "\n",
    "def calculate_kl(d1,d2):\n",
    "    kl = 0\n",
    "    for k in d1.keys():\n",
    "        kl += d1[k] * math.log(d1[k]/d2[k])\n",
    "    return kl\n",
    "        \n",
    "\n",
    "def calculate_norm_kl(kmer, egs, e_dnesa, rgs, r_dnesa, seq_lengths, nof_peaks):\n",
    "    es, ef = e_dnesa.search(kmer)\n",
    "    rs, rf = r_dnesa.search(kmer)\n",
    "    if es and rs:\n",
    "        d1 = make_prob_distr( add_unzeros( m_dsitr_to_seq_freq( get_m_distr( e_dnesa, es, ef ), seq_lengths, len(kmer) ), nof_peaks) )\n",
    "        d2 = make_prob_distr( add_unzeros( m_dsitr_to_seq_freq( get_m_distr( r_dnesa, rs, rf ), seq_lengths, len(kmer) ), nof_peaks) )\n",
    "\n",
    "        dd = list()\n",
    "        for k,v in d1.items():\n",
    "            dd.append( (v,k) )\n",
    "        dd = sorted( dd )\n",
    "        d1m = dict()\n",
    "        for i in range(len(dd)):\n",
    "            d1m[ dd[i][1] ] = dd[ len(dd)-i-1 ][0]\n",
    "\n",
    "        return calculate_kl(d1,d2) / calculate_kl(d1,d1m)\n",
    "    return 0.0\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# A comparision on fixed-length word sets using a nucleotide shuffling model\n",
    "\n",
    "In this section we show a comparison between the two scoring procedures $kl\\_rank$ and $norm\\_kl\\_score$ on sets of words of fixed (same) length $k$, by taking into account the nucleotide shuffling model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epeaks, es2n, egs = read_global_sequence('ENCFF682EOV_allseqs.fa')\n",
    "edf = pandas.read_csv('ENCFF682EOV_allseqs.nelsa.csv')\n",
    "print(len(epeaks), len(es2n), len(egs))\n",
    "e_dnesa = DNESA(list(edf.sa), list(edf.lcp), list(edf.nn), list(edf.dl), egs)\n",
    "\n",
    "rpeaks, rs2n, rgs = read_global_sequence('ENCFF682EOV_allseqs_shuff.fa')\n",
    "rdf = pandas.read_csv('ENCFF682EOV_allseqs_shuff.nelsa.csv')\n",
    "print(len(rpeaks), len(rs2n), len(rgs))\n",
    "r_dnesa = DNESA(list(rdf.sa), list(rdf.lcp), list(rdf.nn), list(rdf.dl), rgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctcf = read_jaspar('MA0139.1.jaspar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition a symmetric KL distance and a generalized Jaccard similarity calculated on top of DL vectors are taken into accont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calulcate_generalized_jaccard(d1, d2):\n",
    "    nom = 0.0\n",
    "    den = 0.0\n",
    "    for k in (d1.keys()|d2.keys()):\n",
    "        nom += max( d1.get(k,0), d2.get(k,0) )\n",
    "        den += min( d1.get(k,0), d2.get(k,0) )\n",
    "    return nom/den\n",
    "\n",
    "def calculate_symmetric_kl(d1, d2):\n",
    "    return (calculate_kl(d1,d2)+calculate_kl(d2,d1))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "\n",
    "kl_rank, norm_kl_rank =  get_kl_scores(k, egs, e_dnesa, rgs, r_dnesa)\n",
    "#print(kl_rank)\n",
    "\n",
    "seq_lengths = dict()\n",
    "for i in range(len(epeaks)):\n",
    "    seq_lengths[i] = len(epeaks[i][0])\n",
    "nof_peaks = len(epeaks)\n",
    "\n",
    "mult = list()\n",
    "dl = list()\n",
    "norm_pwm_score = list()\n",
    "norm_kl = list()\n",
    "symmetric_kl = list()\n",
    "jaccard = list()\n",
    "\n",
    "for s in kl_rank:\n",
    "    kmer = s[1]\n",
    "    \n",
    "    norm_pwm_score.append( get_normalized_motif_score(ctcf, kmer)[0] )\n",
    "    norm_kl.append( calculate_norm_kl(kmer, egs, e_dnesa, rgs, r_dnesa, seq_lengths, nof_peaks ) )\n",
    "    ks, ke = e_dnesa.search(kmer)\n",
    "    if ks:\n",
    "        rs, rf = r_dnesa.search(kmer)\n",
    "        if rs:\n",
    "            d1 = get_m_distr( e_dnesa, ks, ke )\n",
    "            d2 = get_m_distr( r_dnesa, rs, rf )\n",
    "            jaccard.append(calulcate_generalized_jaccard(d1,d2))\n",
    "            \n",
    "            d1 = make_prob_distr( add_unzeros( m_dsitr_to_seq_freq( get_m_distr( e_dnesa, ks, ke ), seq_lengths, len(kmer) ), nof_peaks) )\n",
    "            d2 = make_prob_distr( add_unzeros( m_dsitr_to_seq_freq( get_m_distr( r_dnesa, rs, rf ), seq_lengths, len(kmer) ), nof_peaks) )\n",
    "            symmetric_kl.append( calculate_symmetric_kl(d1,d2) )\n",
    "        else:\n",
    "            symmetric_kl.append(0)\n",
    "            jaccard.append(0)\n",
    "        \n",
    "        mult.append(ke-ks)\n",
    "        dl.append( len(set(edf.dl[ks:ke])))\n",
    "    else:\n",
    "        mult.append(0)\n",
    "        dl.append(0)\n",
    "        symmetric_kl.append(0)\n",
    "        jaccard.append(0)\n",
    "        \n",
    "import pandas\n",
    "exp1_df = pandas.DataFrame()\n",
    "exp1_df['kmer'] = [ s[1] for s in kl_rank ]\n",
    "exp1_df['norm_pwm_score'] = norm_pwm_score\n",
    "exp1_df['dl'] = dl\n",
    "exp1_df['mult'] = mult\n",
    "exp1_df['kl_rank'] = [ s[0] for s in kl_rank ]\n",
    "exp1_df['norm_kl_rank'] = [ s[0] for s in norm_kl_rank ]\n",
    "exp1_df['norm_kl_score'] = norm_kl\n",
    "exp1_df['symmetric_kl'] = symmetric_kl\n",
    "exp1_df['jaccard'] = jaccard\n",
    "\n",
    "print('-'*40)\n",
    "display(exp1_df)\n",
    "\n",
    "exp1_df.to_csv('scores.shuffle.k'+str(k)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(exp1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = exp1_df\n",
    "\n",
    "plot_2d_scatter(df, 'dl', 'norm_pwm_score')\n",
    "\n",
    "plot_2d_scatter(df, 'kl_rank', 'norm_pwm_score')\n",
    "plot_2d_scatter(df, 'norm_kl_rank', 'norm_pwm_score')\n",
    "plot_2d_scatter(df, 'norm_kl_score', 'norm_pwm_score')\n",
    "plot_2d_scatter(df, 'symmetric_kl', 'norm_pwm_score')\n",
    "plot_2d_scatter(df, 'jaccard', 'norm_pwm_score')\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,2))\n",
    "corrMatrix = df.corr()\n",
    "sn.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# A comparision on fixed-length word sets using a sequence re-extraction model\n",
    "\n",
    "In this section we show a comparison between the two scoring procedures $kl\\_rank$ and $norm\\_kl\\_score$ on sets of words of fixed (same) length $k$, by taking into account the nucleotide shuffling model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epeaks, es2n, egs = read_global_sequence('ENCFF682EOV_allseqs.fa')\n",
    "edf = pandas.read_csv('ENCFF682EOV_allseqs.nelsa.csv')\n",
    "print(len(epeaks), len(es2n), len(egs))\n",
    "e_dnesa = DNESA(list(edf.sa), list(edf.lcp), list(edf.nn), list(edf.dl), egs)\n",
    "\n",
    "rpeaks, rs2n, rgs = read_global_sequence('ENCFF682EOV_allseqs_random.fa')\n",
    "rdf = pandas.read_csv('ENCFF682EOV_allseqs_random.nelsa.csv')\n",
    "print(len(rpeaks), len(rs2n), len(rgs))\n",
    "r_dnesa = DNESA(list(rdf.sa), list(rdf.lcp), list(rdf.nn), list(rdf.dl), rgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctcf = read_jaspar('MA0139.1.jaspar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition a symmetric KL distance and a generalized Jaccard similarity calculated on top of DL vectors are taken into accont."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "\n",
    "kl_rank, norm_kl_rank =  get_kl_scores(k, egs, e_dnesa, rgs, r_dnesa)\n",
    "#print(kl_rank)\n",
    "\n",
    "seq_lengths = dict()\n",
    "for i in range(len(epeaks)):\n",
    "    seq_lengths[i] = len(epeaks[i][0])\n",
    "nof_peaks = len(epeaks)\n",
    "\n",
    "mult = list()\n",
    "dl = list()\n",
    "norm_pwm_score = list()\n",
    "norm_kl = list()\n",
    "symmetric_kl = list()\n",
    "jaccard = list()\n",
    "\n",
    "for s in kl_rank:\n",
    "    kmer = s[1]\n",
    "    \n",
    "    norm_pwm_score.append( get_normalized_motif_score(ctcf, kmer)[0] )\n",
    "    norm_kl.append( calculate_norm_kl(kmer, egs, e_dnesa, rgs, r_dnesa, seq_lengths, nof_peaks ) )\n",
    "    ks, ke = e_dnesa.search(kmer)\n",
    "    if ks:\n",
    "        rs, rf = r_dnesa.search(kmer)\n",
    "        if rs:\n",
    "            d1 = get_m_distr( e_dnesa, ks, ke )\n",
    "            d2 = get_m_distr( r_dnesa, rs, rf )\n",
    "            jaccard.append(calulcate_generalized_jaccard(d1,d2))\n",
    "            \n",
    "            d1 = make_prob_distr( add_unzeros( m_dsitr_to_seq_freq( get_m_distr( e_dnesa, ks, ke ), seq_lengths, len(kmer) ), nof_peaks) )\n",
    "            d2 = make_prob_distr( add_unzeros( m_dsitr_to_seq_freq( get_m_distr( r_dnesa, rs, rf ), seq_lengths, len(kmer) ), nof_peaks) )\n",
    "            symmetric_kl.append( calculate_symmetric_kl(d1,d2) )\n",
    "        else:\n",
    "            symmetric_kl.append(0)\n",
    "            jaccard.append(0)\n",
    "        \n",
    "        mult.append(ke-ks)\n",
    "        dl.append( len(set(edf.dl[ks:ke])))\n",
    "    else:\n",
    "        mult.append(0)\n",
    "        dl.append(0)\n",
    "        symmetric_kl.append(0)\n",
    "        jaccard.append(0)\n",
    "        \n",
    "import pandas\n",
    "exp1_df = pandas.DataFrame()\n",
    "exp1_df['kmer'] = [ s[1] for s in kl_rank ]\n",
    "exp1_df['norm_pwm_score'] = norm_pwm_score\n",
    "exp1_df['dl'] = dl\n",
    "exp1_df['mult'] = mult\n",
    "exp1_df['kl_rank'] = [ s[0] for s in kl_rank ]\n",
    "exp1_df['norm_kl_rank'] = [ s[0] for s in norm_kl_rank ]\n",
    "exp1_df['norm_kl_score'] = norm_kl\n",
    "exp1_df['symmetric_kl'] = symmetric_kl\n",
    "exp1_df['jaccard'] = jaccard\n",
    "\n",
    "print('-'*40)\n",
    "display(exp1_df)\n",
    "\n",
    "exp1_df.to_csv('scores.re-extract.k'+str(k)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = exp1_df\n",
    "\n",
    "plot_2d_scatter(df, 'dl', 'norm_pwm_score')\n",
    "\n",
    "plot_2d_scatter(df, 'kl_rank', 'norm_pwm_score')\n",
    "plot_2d_scatter(df, 'norm_kl_rank', 'norm_pwm_score')\n",
    "plot_2d_scatter(df, 'norm_kl_score', 'norm_pwm_score')\n",
    "plot_2d_scatter(df, 'symmetric_kl', 'norm_pwm_score')\n",
    "plot_2d_scatter(df, 'jaccard', 'norm_pwm_score')\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,2))\n",
    "corrMatrix = df.corr()\n",
    "sn.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# A simple procedure to extract maximal words given a DL threshold\n",
    "\n",
    "A simple procedure to extract word as long as possible such that they are present in a certain percentage of sequences is given in what follows.<br>\n",
    "The aim of the procedure is to extend a given word $w$ ultil the support threshold is satisfied.<br>\n",
    "Thus, given a threshold $t$, which represent a minimum number of sequences where words must occurr, and a word $w$ such that $dl(w,S)\\geq t$,  $w$ is considered maximal if there not exist an elongation of $w$, $wx : x \\in \\{A,C,G,T\\}$, such that $dl(wx,S)\\geq t$.<br>\n",
    "Maximal words are extracted by the algorithm. In contrast, words that satisfy the threshold but that are not maximal, are discarded and their elogations are taken into account.<br>\n",
    "The procedure is a recursive algoritm which starts form 1-mers and elongate them until maximal words are reached. If 1-mers are considered as roots of a prefix tree, it can be shown that this bottom-up procedure extracts the same words of a top-down procedure which, in constrats, works by aggregating *unfrequent* words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intervals(istart, iend, k, dnesa):\n",
    "    \"\"\"\n",
    "    Given a word length k, a DNESA, and an initial lcp-interval with coordinates istar-iend which represent a k-mer w, \n",
    "    the procedures searches the lcp-interval of the 4 elongations wA, wC, wG and wT.\n",
    "    The procedure is inefficient since it can be done by a dicotomic seach rather than a linear scan.\n",
    "    \"\"\"\n",
    "    intervals = list()\n",
    "    \n",
    "    rstart = istart\n",
    "    while (rstart < iend) and (dnesa.nn[rstart] <= k) :\n",
    "        rstart += 1\n",
    "    \n",
    "    while rstart < iend:\n",
    "        rend = rstart + 1\n",
    "        c = dnesa.gs[ dnesa.sa[rstart] +k ]\n",
    "        while (rend < iend) and (dnesa.nn[ rend ] >=k ) and ( dnesa.gs[ dnesa.sa[rend] +k ] == c ):\n",
    "            rend += 1\n",
    "        intervals.append( (c, rstart, rend) )\n",
    "        rstart = rend\n",
    "    return intervals\n",
    "\n",
    "\n",
    "#good_words = list()\n",
    "def up_until_dl_thr(w, istart, iend, k, cdl, dl_thr, dnesa, good_words):\n",
    "    \"\"\"\n",
    "    A recursive procedure to extend a word w into an extension wX until dl(wX,S) >= dl_thr, the input support threshold.\n",
    "    Found maximal words are added up to the list good_words\n",
    "    \"\"\"\n",
    "    intervals = get_intervals(istart, iend, k+1, dnesa)\n",
    "    #print(intervals)\n",
    "\n",
    "    good_intervals = list()\n",
    "    for interval in intervals:\n",
    "        idl = len(set( dnesa.dl[interval[1]: interval[2]] ))\n",
    "        if idl >= dl_thr:\n",
    "            good_intervals.append((interval, idl))\n",
    "\n",
    "    #print(good_intervals)\n",
    "\n",
    "    if len(good_intervals)>0:\n",
    "        for interval in good_intervals:\n",
    "            up_until_dl_thr(  w+interval[0][0], interval[0][1], interval[0][2], k+1, interval[1], dl_thr, dnesa, good_words )\n",
    "    else:\n",
    "        #print(w, cdl)\n",
    "        good_words.append( (w,istart,iend) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support threshold is named *dl_thr_perc* and it is defined as a percentage of the input set $S$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def extract_maximal_dl_words(iseqs, inelsa, dl_thr_perc):\n",
    "    # we firstly read the input set S and the DNESA data structure\n",
    "    epeaks, es2n, egs = read_global_sequence(iseqs)\n",
    "    edf = pandas.read_csv(inelsa)\n",
    "    print(len(epeaks), len(es2n), len(egs))\n",
    "    e_dnesa = DNESA(list(edf.sa), list(edf.lcp), list(edf.nn), list(edf.dl), egs)\n",
    "\n",
    "\n",
    "    total_dl = len(set(e_dnesa.dl))\n",
    "    print('total peaks',total_dl)\n",
    "\n",
    "    #dl_thr_perc = 0.6  #<---- this is the threshold parameter\n",
    "    dl_thr = math.ceil( total_dl*dl_thr_perc )\n",
    "    print('thr',dl_thr_perc, '=', dl_thr)\n",
    "\n",
    "\n",
    "    istart = 0\n",
    "    iend = len(e_dnesa.gs)\n",
    "\n",
    "    print('getting 0 intervals')\n",
    "    intervals = get_intervals(istart, iend, 0, e_dnesa)\n",
    "    print(intervals)\n",
    "\n",
    "    good_words = list() \n",
    "    for interval in intervals:\n",
    "        print(interval)\n",
    "        up_until_dl_thr( interval[0], interval[1], interval[2], 0,  len(set(e_dnesa.dl[interval[1]:interval[2]])), dl_thr, e_dnesa, good_words  )\n",
    "        \n",
    "    return good_words\n",
    "\n",
    "\n",
    "good_words = extract_maximal_dl_words('ENCFF682EOV_allseqs.fa', 'ENCFF682EOV_allseqs.nelsa.csv', 0.8)\n",
    "print(len(good_words),'extracted words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted([ s[0] for s in good_words]))\n",
    "print(len(good_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---- \n",
    "\n",
    "# A comparison on variable-length words, with the sequence re-extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epeaks, es2n, egs = read_global_sequence('ENCFF682EOV_allseqs.fa')\n",
    "edf = pandas.read_csv('ENCFF682EOV_allseqs.nelsa.csv')\n",
    "print(len(epeaks), len(es2n), len(egs))\n",
    "e_dnesa = DNESA(list(edf.sa), list(edf.lcp), list(edf.nn), list(edf.dl), egs)\n",
    "\n",
    "rpeaks, rs2n, rgs = read_global_sequence('ENCFF682EOV_allseqs_random.fa')\n",
    "rdf = pandas.read_csv('ENCFF682EOV_allseqs_random.nelsa.csv')\n",
    "print(len(rpeaks), len(rs2n), len(rgs))\n",
    "r_dnesa = DNESA(list(rdf.sa), list(rdf.lcp), list(rdf.nn), list(rdf.dl), rgs)\n",
    "\n",
    "ctcf = read_jaspar('MA0139.1.jaspar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to calculate $kl\\_rank$ for words in a range of lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_kl_rank = dict()\n",
    "d_norm_kl_rank = dict()\n",
    "\n",
    "for k in range(2,7):\n",
    "    print(k)\n",
    "    k_kl_rank, k_norm_kl_rank =  get_kl_scores(k, egs, e_dnesa, rgs, r_dnesa)\n",
    "    for s in k_kl_rank:\n",
    "        d_kl_rank[ s[1] ] = s[0]\n",
    "    for s in k_norm_kl_rank:\n",
    "        d_norm_kl_rank[ s[1] ] = s[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extract the words by usgin the simple procedure based on DL threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_thr = 0.8\n",
    "extracted_words = extract_maximal_dl_words('ENCFF682EOV_allseqs.fa', 'ENCFF682EOV_allseqs.nelsa.csv', dl_thr)\n",
    "print(len(good_words),'extracted words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = dict()\n",
    "for i in range(len(epeaks)):\n",
    "    seq_lengths[i] = len(epeaks[i][0])\n",
    "nof_peaks = len(epeaks)\n",
    "\n",
    "kl_rank = list()\n",
    "norm_kl_rank = list()\n",
    "mult = list()\n",
    "dl = list()\n",
    "norm_pwm_score = list()\n",
    "norm_kl = list()\n",
    "symmetric_kl = list()\n",
    "jaccard = list()\n",
    "\n",
    "for kmer in extracted_words:\n",
    "    kmer = kmer[0]\n",
    "    norm_pwm_score.append( get_normalized_motif_score(ctcf, kmer)[0] )\n",
    "    \n",
    "    kl_rank.append( d_kl_rank.get(kmer, 0) )\n",
    "    norm_kl_rank.append( d_norm_kl_rank.get(kmer, 0) )\n",
    "    \n",
    "    norm_kl.append( calculate_norm_kl(kmer, egs, e_dnesa, rgs, r_dnesa, seq_lengths, nof_peaks ) )\n",
    "    \n",
    "    ks, ke = e_dnesa.search(kmer)\n",
    "    if ks:\n",
    "        rs, rf = r_dnesa.search(kmer)\n",
    "        if rs:\n",
    "            d1 = get_m_distr( e_dnesa, ks, ke )\n",
    "            d2 = get_m_distr( r_dnesa, rs, rf )\n",
    "            jaccard.append(calulcate_generalized_jaccard(d1,d2))\n",
    "            \n",
    "            d1 = make_prob_distr( add_unzeros( m_dsitr_to_seq_freq( get_m_distr( e_dnesa, ks, ke ), seq_lengths, len(kmer) ), nof_peaks) )\n",
    "            d2 = make_prob_distr( add_unzeros( m_dsitr_to_seq_freq( get_m_distr( r_dnesa, rs, rf ), seq_lengths, len(kmer) ), nof_peaks) )\n",
    "            symmetric_kl.append( calculate_symmetric_kl(d1,d2) )\n",
    "        else:\n",
    "            symmetric_kl.append(0)\n",
    "            jaccard.append(0)\n",
    "        \n",
    "        mult.append(ke-ks)\n",
    "        dl.append( len(set(edf.dl[ks:ke])))\n",
    "    else:\n",
    "        mult.append(0)\n",
    "        dl.append(0)\n",
    "        symmetric_kl.append(0)\n",
    "        jaccard.append(0)\n",
    "        \n",
    "import pandas\n",
    "exp1_df = pandas.DataFrame()\n",
    "exp1_df['kmer'] = [kmer[0] for kmer in extracted_words]\n",
    "exp1_df['norm_pwm_score'] = norm_pwm_score\n",
    "exp1_df['dl'] = dl\n",
    "exp1_df['mult'] = mult\n",
    "exp1_df['kl_rank'] = kl_rank\n",
    "exp1_df['norm_kl_rank'] = norm_kl_rank\n",
    "exp1_df['norm_kl_score'] = norm_kl\n",
    "exp1_df['symmetric_kl'] = symmetric_kl\n",
    "exp1_df['jaccard'] = jaccard\n",
    "\n",
    "print('-'*40)\n",
    "display(exp1_df)\n",
    "\n",
    "exp1_df.to_csv('scores.re-extract.simple-maximal.'+str(dl_thr)+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = exp1_df\n",
    "\n",
    "plot_2d_scatter(df, 'dl', 'norm_pwm_score')\n",
    "\n",
    "plot_2d_scatter(df, 'kl_rank', 'norm_pwm_score')\n",
    "plot_2d_scatter(df, 'norm_kl_rank', 'norm_pwm_score')\n",
    "plot_2d_scatter(df, 'norm_kl_score', 'norm_pwm_score')\n",
    "plot_2d_scatter(df, 'symmetric_kl', 'norm_pwm_score')\n",
    "plot_2d_scatter(df, 'jaccard', 'norm_pwm_score')\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(5,2))\n",
    "corrMatrix = df.corr()\n",
    "sn.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
